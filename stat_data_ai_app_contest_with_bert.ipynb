{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhpCIij5JHE7"
      },
      "source": [
        "Last Update @ 2020.12.04\n",
        "\n",
        "- Huggingface Transformers 4.0.0  버전 반영"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGAdyjAdF0kd"
      },
      "source": [
        "# Package 설치 & 데이터 받기"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%bash\n",
        "# apt-get update\n",
        "# apt-get install g++ openjdk-8-jdk python-dev python3-dev\n",
        "# pip3 install JPype1\n",
        "# pip3 install konlpy"
      ],
      "metadata": {
        "id": "4l1X01G9Gg8C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %env JAVA_HOME \"/usr/lib/jvm/java-8-openjdk-amd64\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AI_kCMSyNuUl",
        "outputId": "5eb14af9-d76d-4ee1-d5a4-1a7ac18a15f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: JAVA_HOME=\"/usr/lib/jvm/java-8-openjdk-amd64\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%bash\n",
        "# bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)\n",
        "# pip3 install /tmp/mecab-python-0.996"
      ],
      "metadata": {
        "id": "9IDUe9Y2NwJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, sys\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ADUHp3WbURCV",
        "outputId": "f94fa638-52c7-4e60-c04f-e4cb003faf36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_path = '/content/notebooks'\n",
        "save_path = '/content/drive/MyDrive/Colab Notebooks/package' ## 패키지가 저장될 경로\n",
        "\n",
        "os.symlink(save_path, my_path)\n",
        "sys.path.insert(0, my_path)"
      ],
      "metadata": {
        "id": "GWtOtLR3UZC2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install --target=$my_path mxnet # 이때, 원하는 경로에 설치하기 위해 --target 옵션을 지정해줍니다.\n",
        "\n",
        "!pip install --target=$my_path mxnet\n",
        "!pip install --target=$my_path gluonnlp pandas tqdm\n",
        "!pip install --target=$my_path sentencepiece\n",
        "!pip install --target=$my_pathtransformers"
      ],
      "metadata": {
        "id": "8O6OMm0-UpEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, sys\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lOMzQYrYWoq1",
        "outputId": "d2ff6849-176c-45a9-b502-d8ed6d694f5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_path = '/content/notebooks'\n",
        "save_path = '/content/drive/MyDrive/Colab Notebooks/package' \n",
        "\n",
        "os.symlink(save_path, my_path)\n",
        "sys.path.insert(0, my_path)"
      ],
      "metadata": {
        "id": "knBw-M6cWem-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizerFast, AlbertModel, BertModel, AutoTokenizer"
      ],
      "metadata": {
        "id": "-q1m1qt0WefV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install hanja\n",
        "!pip install mxnet\n",
        "!pip install gluonnlp pandas tqdm\n",
        "!pip install sentencepiece\n",
        "!pip install  transformers\n",
        "!pip install torch\n",
        "!pip install hanja\n",
        "!pip install imblearn\n",
        "!pip install WordCloud\n",
        "!apt-get install fonts-nanum*\n",
        "!apt-get install fontconfig"
      ],
      "metadata": {
        "id": "hlkauFC9UM_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizerFast, AlbertModel, BertModel, AutoTokenizer\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import hanja\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "from transformers.optimization import get_cosine_schedule_with_warmup, get_linear_schedule_with_warmup\n",
        "from transformers import AdamW\n",
        "\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "15imiRmNgevJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eM95Pr7zGiiW",
        "outputId": "c677b91e-999c-4bd0-f5b3-c48c421b3cdc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('/content/drive/MyDrive/temp/train_data.csv', encoding='cp949')\n",
        "data.title=data.title.astype(str)\n",
        "\n",
        "val_dataset = pd.read_csv('/content/drive/MyDrive/temp/valid.csv', encoding='cp949')\n",
        "val_dataset.title=val_dataset.title.astype(str)"
      ],
      "metadata": {
        "id": "8IfpPsfddtRJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_val, Y_label1_train, Y_label1_val =  train_test_split(data, test_size = 0.2, random_state = 42)"
      ],
      "metadata": {
        "id": "5ili1OOyyGRz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 뉴스 데이터 클래스 생성\n",
        "class NewsSubjectDataset(Dataset):\n",
        "  def __init__(self, subjects, targets, tokenizer, max_len):\n",
        "    self.subjects = subjects\n",
        "    self.targets = targets\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_len = max_len\n",
        "  def __len__(self):\n",
        "    return len(self.subjects)\n",
        "  def __getitem__(self, item):\n",
        "    subject = str(self.subjects[item])\n",
        "    target = self.targets[item]\n",
        "    encoding = self.tokenizer.encode_plus(\n",
        "      subject,\n",
        "      add_special_tokens=True,\n",
        "      max_length=self.max_len,\n",
        "      return_token_type_ids=False,\n",
        "      padding = 'max_length',\n",
        "      truncation = True,\n",
        "      return_attention_mask=True,\n",
        "      return_tensors='pt',\n",
        "    )\n",
        "    return {\n",
        "      'subject_text': subject,\n",
        "      'input_ids': encoding['input_ids'].flatten(),\n",
        "      'attention_mask': encoding['attention_mask'].flatten(),\n",
        "      'targets': torch.tensor(target, dtype=torch.long)\n",
        "    }\n",
        "def create_data_loader(df, tokenizer, max_len, batch_size, shuffle_=False, valid=False):\n",
        "  if valid:\n",
        "    ds = NewsSubjectDataset(\n",
        "      subjects=df.title.to_numpy(),\n",
        "      targets=np.zeros(len(df)),\n",
        "      tokenizer=tokenizer,\n",
        "      max_len=max_len\n",
        "      )\n",
        "  else:\n",
        "    ds = NewsSubjectDataset(\n",
        "      subjects=df.title.to_numpy(),\n",
        "      targets=df.topic_idx.to_numpy(),\n",
        "      tokenizer=tokenizer,\n",
        "      max_len=max_len\n",
        "    )\n",
        "  return DataLoader(\n",
        "    ds,\n",
        "    batch_size=batch_size,\n",
        "    num_workers=4,\n",
        "    shuffle = shuffle_\n",
        "  )"
      ],
      "metadata": {
        "id": "V6gmotZsZ9j-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 로더 생성\n",
        "BATCH_SIZE = 128\n",
        "MAX_LEN = 16"
      ],
      "metadata": {
        "id": "WjgnevRJk3Lv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "titles_t = data.title.to_numpy().reshape(-1,1)\n",
        "labels_t = data.topic_idx.to_numpy().reshape(-1,1)\n",
        "# oversample = RandomOverSampler()\n",
        "# X_over, y_over = oversample.fit_resample(titles_t, labels_t)\n",
        "train = pd.DataFrame({'title':titles_t.reshape(-1), 'topic_idx':labels_t.reshape(-1)})\n",
        "tokenizer_kcbert = AutoTokenizer.from_pretrained(\"beomi/kcbert-large\")\n",
        "train_data_loader = create_data_loader(train, tokenizer_kcbert, MAX_LEN, BATCH_SIZE, shuffle_=True)\n",
        "valid_data_loader = create_data_loader(val_dataset, tokenizer_kcbert, MAX_LEN, BATCH_SIZE, valid=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NCjwj9DShZC7",
        "outputId": "dbe59536-82f6-48ae-c51d-ff31e64d4954"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def remove_and_substitute(title):\n",
        "#   x = title[:]\n",
        "#   x = re.sub('\\.\\.\\.', '', x)\n",
        "#   x = re.sub('[^ㄱ-ㅎㅏ-ㅣ가-힣a-zA-Z\\s]', ' ', x)\n",
        "#   x = hanja.translate(x, 'substitution')\n",
        "#   return x.strip()\n",
        "  \n",
        "# k = train['title'].apply(remove_and_substitute)\n",
        "# train.title = k\n",
        "# k = df_valid['title'].apply(remove_and_substitute)\n",
        "# df_valid.title = k"
      ],
      "metadata": {
        "id": "SqYkYTk5hoPM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NewsSubjectClassifier_kcbert(nn.Module):\n",
        "  def __init__(self, n_classes):\n",
        "    super(NewsSubjectClassifier_kcbert, self).__init__()\n",
        "    self.bert = BertModel.from_pretrained(\"beomi/kcbert-large\")\n",
        "    self.drop = nn.Dropout(p=0.5)\n",
        "    self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
        "  def forward(self, input_ids, attention_mask):\n",
        "    _, pooled_output = self.bert(\n",
        "      input_ids=input_ids,\n",
        "      attention_mask=attention_mask,\n",
        "       return_dict=False\n",
        "    )\n",
        "    output = self.drop(pooled_output)\n",
        "    return self.out(output)"
      ],
      "metadata": {
        "id": "R9T10_TSh6D_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\")\n",
        "EPOCHS = 10\n",
        "loss_fn = nn.CrossEntropyLoss().to(device)"
      ],
      "metadata": {
        "id": "ibGo76s4h89Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(model,data_loader,loss_fn,optimizer,device,scheduler,n_examples):\n",
        "  model = model.train()\n",
        "  losses = []\n",
        "  correct_predictions = 0\n",
        "  for ii, d in tqdm(enumerate(data_loader)):\n",
        "    input_ids = d[\"input_ids\"].to(device)\n",
        "    attention_mask = d[\"attention_mask\"].to(device)\n",
        "    targets = d[\"targets\"].to(device)\n",
        "    outputs = model(\n",
        "      input_ids=input_ids,\n",
        "      attention_mask=attention_mask\n",
        "    )\n",
        "    _, preds = torch.max(outputs, dim=1)\n",
        "    loss = loss_fn(outputs, targets)\n",
        "    correct_predictions += torch.sum(preds == targets)\n",
        "    losses.append(loss.item())\n",
        "    loss.backward()\n",
        "    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    optimizer.zero_grad()\n",
        "    if ii % 100 == 2 :\n",
        "        print(f'Train accuracy {correct_predictions.double() / len(train)} , loss {np.mean(losses)}', ii, \"th\")\n",
        "\n",
        "  return correct_predictions.double() / n_examples, np.mean(losses) #train_acc, train_loss"
      ],
      "metadata": {
        "id": "5SF6AJZ8h9td"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_predictions(model, data_loader):\n",
        "  model = model.eval()\n",
        "  subject_texts = []\n",
        "  predictions = []\n",
        "  prediction_probs = []\n",
        "  with torch.no_grad():\n",
        "    for ii, d in enumerate(data_loader):\n",
        "      texts = d[\"subject_text\"]\n",
        "      input_ids = d[\"input_ids\"].to(device)\n",
        "      attention_mask = d[\"attention_mask\"].to(device)\n",
        "      targets = d[\"targets\"].to(device)\n",
        "      outputs = model(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask\n",
        "      )\n",
        "      _, preds = torch.max(outputs, dim=1)\n",
        "      subject_texts.extend(texts)\n",
        "      predictions.extend(preds)\n",
        "      prediction_probs.extend(outputs)\n",
        "  predictions = torch.stack(predictions).cpu()\n",
        "  prediction_probs = torch.stack(prediction_probs).cpu()\n",
        "  return subject_texts, predictions, prediction_probs\n",
        "import gc"
      ],
      "metadata": {
        "id": "IMRUWTm8ySUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_kcbert = NewsSubjectClassifier_kcbert(n_classes=19).to(device)\n",
        "optimizer = AdamW(model_kcbert.parameters(), lr=1e-4)\n",
        "total_steps = len(train_data_loader) * EPOCHS\n",
        "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=int(total_steps*0.1), num_training_steps=total_steps)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iqRjARm-h_xz",
        "outputId": "d45e13dd-93f2-4947-f202-4f6260729e56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at beomi/kcbert-large were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 5"
      ],
      "metadata": {
        "id": "yP2wYPgE4FeK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(EPOCHS):\n",
        "  print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
        "  print('-' * 10)\n",
        "  train_acc, train_loss = train_epoch(\n",
        "    model_kcbert,\n",
        "    train_data_loader,\n",
        "    loss_fn,\n",
        "    optimizer,\n",
        "    device,\n",
        "    scheduler,\n",
        "    len(train)\n",
        "  )\n",
        "  print(f'Train loss {train_loss} accuracy {train_acc}')"
      ],
      "metadata": {
        "id": "E_NSQJSFiFll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model2 = model_kcbert.eval()\n",
        "subject_texts = []\n",
        "predictions = []\n",
        "prediction_probs = []\n",
        "with torch.no_grad():\n",
        "    for ii, d in tqdm(enumerate(valid_data_loader)):\n",
        "        texts = d[\"subject_text\"]\n",
        "        input_ids = d[\"input_ids\"].to(device)\n",
        "        attention_mask = d[\"attention_mask\"].to(device)\n",
        "        targets = d[\"targets\"].to(device)\n",
        "        outputs = model2(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask\n",
        "        )\n",
        "        _, preds = torch.max(outputs, dim=1)\n",
        "        subject_texts.extend(texts)\n",
        "        predictions.extend(preds)\n",
        "        prediction_probs.extend(outputs)\n",
        "predictions = torch.stack(predictions).cpu()\n",
        "prediction_probs = torch.stack(prediction_probs).cpu()"
      ],
      "metadata": {
        "id": "CZj2yHiu0V53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# >>> a = torch.randn(4, 4)\n",
        "# >>> a\n",
        "# tensor([[-1.2360, -0.2942, -0.1222,  0.8475],\n",
        "#         [ 1.1949, -1.1127, -2.2379, -0.6702],\n",
        "#         [ 1.5717, -0.9207,  0.1297, -1.8768],\n",
        "#         [-0.6172,  1.0036, -0.6060, -0.2432]])\n",
        "# >>> torch.max(a, 1)\n",
        "# torch.return_types.max(values=tensor([0.8475, 1.1949, 1.5717, 1.0036]), indices=tensor([3, 0, 0, 1]))"
      ],
      "metadata": {
        "id": "SwTntjS_FZNe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answers = []\n",
        "\n",
        "y_review_texts, y_pred, y_pred_probs = get_predictions(\n",
        "  model_kcbert,\n",
        "  valid_data_loader\n",
        ")\n",
        "\n",
        "answers.append(y_pred.tolist())"
      ],
      "metadata": {
        "id": "-58ettGfwIKt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}