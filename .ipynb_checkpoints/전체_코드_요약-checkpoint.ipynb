{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a7d24d6",
   "metadata": {},
   "source": [
    "# 패키지 모음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5725f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(os.getcwd() + '\\\\data')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pandas import DataFrame, get_dummies\n",
    "\n",
    "#import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from konlpy.tag import Komoran "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54ad0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cbc34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa \n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.models import Sequential\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "from keras.layers import Input, Dense, Embedding, SpatialDropout1D # , add, concatenate\n",
    "from keras.layers import LSTM, Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D, GRU, CuDNNLSTM, CuDNNGRU\n",
    "from keras.layers import BatchNormalization, Activation\n",
    "\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9387e7e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-08T14:19:43.306169Z",
     "start_time": "2022-08-08T14:19:43.293166Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a239d77d",
   "metadata": {},
   "source": [
    "# 데이터 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a92e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_c = pd.read_csv(\"1. 실습용자료.txt\",sep='|',encoding='cp949')\n",
    "\n",
    "first = first_c.copy()\n",
    "del first['AI_id']\n",
    "\n",
    "first = first.drop_duplicates(keep='first').reset_index(drop=True)\n",
    "\n",
    "first = first.fillna('')\n",
    "\n",
    "first['total'] = first['text_obj'] + ' ' + first['text_mthd'] + ' ' + first['text_deal']\n",
    "first['total'] = first['total'].apply((lambda x: re.sub(',',' ',x)))\n",
    "first['total'] = first['total'].apply((lambda x: re.sub('-',' ',x)))\n",
    "first['total'] = first['total'].apply((lambda x: re.sub('[^a-zA-z0-9\\s가-힣]',' ',x)))\n",
    "first['total'] = first['total'].apply((lambda x: re.sub('  ',' ',x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725d979d",
   "metadata": {},
   "outputs": [],
   "source": [
    "komoran = Komoran()\n",
    "\n",
    "stop_words = list(pd.read_csv(\"stopwords.txt\",header=None, names=['stop'])['stop'])\n",
    "\n",
    "first['nouns'] = [ ' '.join([w for w in komoran.nouns(temp) if w not in stop_words]) for temp in tqdm(first['total']) ] \n",
    "\n",
    "first.to_csv(\"eda_complete.csv\",index=False, encoding = 'cp949')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1a5c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "first = pd.read_csv(\"eda_complete.csv\", encoding = 'cp949')\n",
    "first.nouns=first.nouns.astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df84bec0",
   "metadata": {},
   "source": [
    "# 모델링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038bc186",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 128\n",
    "lstm_out = 196\n",
    "\n",
    "# earlyStop=EarlyStopping(monitor=\"val_accuracy\",mode = 'max', patience=3, verbose=1) #최소 3번 이상의 에포치 동안 개선 없을시\n",
    "earlyStop=EarlyStopping(monitor=\"val_accuracy\",patience=10, verbose=1) #최소 3번 이상의 에포치 동안 개선 없을시\n",
    "\n",
    "adam = Adam(\n",
    "    learning_rate=0.0001, # default = 0.001 (줄인게 효과는 있어보임)\n",
    "    beta_1=0.9,\n",
    "    beta_2=0.999,\n",
    "    epsilon=1e-07,\n",
    "    amsgrad=False,\n",
    "    name='Adam'\n",
    ")\n",
    "\n",
    "label1_f1 = tfa.metrics.F1Score(19,'macro')\n",
    "label2_f1 = tfa.metrics.F1Score(74,'macro')\n",
    "label3_f1 = tfa.metrics.F1Score(225,'macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375ee49e",
   "metadata": {},
   "source": [
    "## 모델1(19개)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2da4f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_label1 = get_dummies(first['digit_1'])\n",
    "\n",
    "X_train, X_val, Y_label1_train, Y_label1_val =  train_test_split(first['nouns'].values, Y_label1, test_size = 0.2, random_state = 42)\n",
    "\n",
    "max_fatures = 10000\n",
    "tokenizer = Tokenizer(num_words=max_fatures, split=' ') #num_words 단어 개수 제한\n",
    "\n",
    "tokenizer.fit_on_texts(X_train.values)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_train = pad_sequences(X_train, maxlen=28)\n",
    "\n",
    "X_val = tokenizer.texts_to_sequences(X_val)\n",
    "X_val = pad_sequences(X_val, maxlen=28)\n",
    "\n",
    "X_train.shape, X_val.shape, Y_label1_train.shape, Y_label1_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a9b493",
   "metadata": {},
   "outputs": [],
   "source": [
    "#집 버전\n",
    "\n",
    "label1 = Sequential()\n",
    "label1.add(Embedding(max_fatures, embed_dim,input_length = X.shape[1]))\n",
    "# label1.add(Conv1D(filters=64, kernel_size=3, padding='same', activation='relu'))\n",
    "# label1.add(MaxPooling1D(pool_size=2))\n",
    "label1.add(SpatialDropout1D(0.4))\n",
    "# label1.add(GRU(lstm_out, dropout=0.2, recurrent_dropout=0.2))\n",
    "label1.add(CuDNNLSTM(lstm_out))\n",
    "label1.add(Dense(units=19, activation = 'softmax'))\n",
    "label1.compile(loss = 'categorical_crossentropy', optimizer=adam, metrics = ['accuracy', label1_f1])\n",
    "\n",
    "score = label1.fit(X_train, Y_label1_train, epochs = 500, batch_size=1024, validation_data=(X_val, Y_label1_val), \n",
    "                   callbacks=[earlyStop], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd1fbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = label1.predict(X_val, verbose=0)\n",
    "y_pred = [i.argmax() for i in temp]\n",
    "\n",
    "temp2 = Y_label1_val.reset_index(drop=True)\n",
    "y_val2 = []\n",
    "for i in tqdm(range(temp2.shape[0])):\n",
    "    y_val2.append(temp2.iloc[i].argmax())\n",
    "    \n",
    "print(\"val accuracy : \", accuracy_score(y_val2, y_pred))\n",
    "\n",
    "print(\"val f1 score : \", f1_score(y_val2, y_pred, average = 'macro')) #일반적인"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359b578f",
   "metadata": {},
   "source": [
    "## 모델2(74개)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d5382d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_label2 = get_dummies(first['digit_2'])\n",
    "\n",
    "X_train, X_val, Y_label2_train, Y_label2_val =  train_test_split(first['nouns'].values, Y_label2, test_size = 0.2, random_state = 42)\n",
    "\n",
    "max_fatures = 10000\n",
    "tokenizer = Tokenizer(num_words=max_fatures, split=' ') #num_words 단어 개수 제한\n",
    "\n",
    "tokenizer.fit_on_texts(X_train.values)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_train = pad_sequences(X_train, maxlen=28)\n",
    "\n",
    "X_val = tokenizer.texts_to_sequences(X_val)\n",
    "X_val = pad_sequences(X_val, maxlen=28)\n",
    "\n",
    "X_train.shape, X_val.shape, Y_label2_train.shape, Y_label2_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b796533",
   "metadata": {},
   "outputs": [],
   "source": [
    "#집 버전\n",
    "\n",
    "label2 = Sequential()\n",
    "label2.add(Embedding(max_fatures, embed_dim,input_length = X.shape[1]))\n",
    "# label2.add(Conv1D(filters=64, kernel_size=3, padding='same', activation='relu'))\n",
    "# label2.add(MaxPooling1D(pool_size=2))\n",
    "label2.add(SpatialDropout1D(0.4))\n",
    "# label2.add(GRU(lstm_out, dropout=0.2, recurrent_dropout=0.2))\n",
    "label2.add(CuDNNLSTM(lstm_out))\n",
    "label2.add(Dense(units=74, activation = 'softmax'))\n",
    "label2.compile(loss = 'categorical_crossentropy', optimizer=adam, metrics = ['accuracy', label2_f1])\n",
    "\n",
    "score = label2.fit(X_train, Y_label2_train, epochs = 500, batch_size=1024, validation_data=(X_val, Y_label2_val), callbacks=[earlyStop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e92fd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_label2_prob = label2.predict(X_val, verbose=0)\n",
    "y_val_label2 = [i.argmax() for i in y_val_label2_prob]\n",
    "y_val_label2 = [Y_label2.columns[i] for i in y_val_label2]\n",
    "\n",
    "temp2 = Y_label2_val.reset_index(drop=True)\n",
    "y_val2 = []\n",
    "for i in tqdm(range(temp2.shape[0])):\n",
    "    y_val2.append(temp2.iloc[i].argmax())\n",
    "y_val2 = [Y_label2.columns[i] for i in y_val2]\n",
    "    \n",
    "print(\"val accuracy : \", accuracy_score(y_val2, y_val_label2))\n",
    "\n",
    "print(\"val f1 score : \", f1_score(y_val2, y_val_label2, average = 'macro')) #일반적인"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c887c92",
   "metadata": {},
   "source": [
    "## 모델3(225개)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318e3ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_label3 = get_dummies(first['digit_3'])\n",
    "\n",
    "X_train, X_val, Y_label3_train, Y_label3_val =  train_test_split(first['nouns'].values, Y_label3, test_size = 0.2, random_state = 42)\n",
    "\n",
    "max_fatures = 10000\n",
    "tokenizer = Tokenizer(num_words=max_fatures, split=' ') #num_words 단어 개수 제한\n",
    "\n",
    "tokenizer.fit_on_texts(X_train.values)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_train = pad_sequences(X_train, maxlen=28)\n",
    "\n",
    "X_val = tokenizer.texts_to_sequences(X_val)\n",
    "X_val = pad_sequences(X_val, maxlen=28)\n",
    "\n",
    "X_train.shape, X_val.shape, Y_label3_train.shape, Y_label3_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6c9468",
   "metadata": {},
   "outputs": [],
   "source": [
    "#집 버전\n",
    "\n",
    "label3 = Sequential()\n",
    "label3.add(Embedding(max_fatures, embed_dim,input_length = X.shape[1]))\n",
    "# label3.add(Conv1D(filters=64, kernel_size=3, padding='same', activation='relu'))\n",
    "# label3.add(MaxPooling1D(pool_size=2))\n",
    "label3.add(SpatialDropout1D(0.4))\n",
    "# label3.add(GRU(lstm_out, dropout=0.2, recurrent_dropout=0.2))\n",
    "label3.add(CuDNNLSTM(lstm_out))\n",
    "label3.add(Dense(units=225, activation = 'softmax'))\n",
    "label3.compile(loss = 'categorical_crossentropy', optimizer=adam, metrics = ['accuracy', label3_f1])\n",
    "\n",
    "score = label3.fit(X_train, Y_label3_train, epochs = 500, batch_size=1024, validation_data=(X_val, Y_label3_val), callbacks=[earlyStop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8643c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_label3_prob = label3.predict(X_val, verbose=0)\n",
    "y_val_label3 = [i.argmax() for i in y_val_label3_prob]\n",
    "y_val_label3 = [Y_label3.columns[i] for i in y_val_label3]\n",
    "\n",
    "temp2 = Y_label3_val.reset_index(drop=True)\n",
    "y_val2 = []\n",
    "for i in tqdm(range(temp2.shape[0])):\n",
    "    y_val2.append(temp2.iloc[i].argmax())\n",
    "y_val2 = [Y_label3.columns[i] for i in y_val2]\n",
    "    \n",
    "print(\"val accuracy : \", accuracy_score(y_val2, y_val_label3))\n",
    "\n",
    "print(\"val f1 score : \", f1_score(y_val2, y_val_label3, average = 'macro')) #일반적인"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d7d884",
   "metadata": {},
   "source": [
    "# test 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65141b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "second = pd.read_csv(\"2. 모델개발용자료.txt\",sep='|',encoding='cp949')\n",
    "\n",
    "second = second.fillna('')\n",
    "\n",
    "second['total'] = second['text_obj'] + ' ' + second['text_mthd'] + ' ' + second['text_deal']\n",
    "second['total'] = second['total'].apply((lambda x: re.sub(',',' ',x)))\n",
    "second['total'] = second['total'].apply((lambda x: re.sub('-',' ',x)))\n",
    "second['total'] = second['total'].apply((lambda x: re.sub('[^a-zA-z0-9\\s가-힣]',' ',x)))\n",
    "second['total'] = second['total'].apply((lambda x: re.sub('  ',' ',x)))\n",
    "\n",
    "second['nouns'] = [ ' '.join([w for w in komoran.nouns(temp) if w not in stop_words]) for temp in tqdm(second['total']) ] \n",
    "\n",
    "X_test = tokenizer.texts_to_sequences(second['nouns'].values)\n",
    "X_test = pad_sequences(X_test,maxlen=28)\n",
    "\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185ec373",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_label1 = label1.predict(X_test, verbose=0)\n",
    "y_test_label1 = [i.argmax() for i in y_test_label1]\n",
    "y_test_label1 = [Y_label1.columns[i] for i in y_test_label1]\n",
    "\n",
    "second['digit_1'] = y_test_label1\n",
    "\n",
    "label1.save(r'models\\lable1_six.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58049026",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_label2 = label2.predict(X_test, verbose=0)\n",
    "y_test_label2 = [i.argmax() for i in y_test_label2]\n",
    "y_test_label2 = [Y_label2.columns[i] for i in y_test_label2]\n",
    "\n",
    "second['digit_2'] = y_test_label2\n",
    "\n",
    "label2.save(r'models\\lable2_six.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5b6050",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_label3 = label3.predict(X_test, verbose=0)\n",
    "y_test_label3 = [i.argmax() for i in y_test_label3]\n",
    "y_test_label3 = [Y_label3.columns[i] for i in y_test_label3]\n",
    "\n",
    "second['digit_3'] = y_test_label3\n",
    "\n",
    "label3.save(r'models\\lable3_six.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5283d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "second.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e9206f",
   "metadata": {},
   "outputs": [],
   "source": [
    "second.to_csv(r\"result\\6차 자료.csv\",index=False, encoding='cp949')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fc7607",
   "metadata": {},
   "source": [
    "# 백업 내용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f8b811",
   "metadata": {},
   "source": [
    "## train, valid 나누고 다시 합치는 내용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0104d84c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-07T00:41:37.262052Z",
     "start_time": "2022-04-07T00:41:36.375856Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((562148,), (140537,), (562148, 74), (140537, 74))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_label2 = get_dummies(first['digit_2'])\n",
    "\n",
    "X_train, X_val, Y_label2_train, Y_label2_val =  train_test_split(first['nouns'], Y_label2, test_size = 0.2, random_state = 42)\n",
    "\n",
    "X_train.shape, X_val.shape, Y_label2_train.shape, Y_label2_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c6ec3fcd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-07T00:34:27.534317Z",
     "start_time": "2022-04-07T00:33:15.687622Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 562148/562148 [01:11<00:00, 7825.03it/s]\n"
     ]
    }
   ],
   "source": [
    "Y_label2_train_int = [Y_label2_train.iloc[i].argmax() for i in tqdm(range(Y_label2_train.shape[0]))]\n",
    "\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "\n",
    "X_train2 = pd.concat([X_train, DataFrame(Y_label2_train_int)], axis=1).reset_index(drop=True)\n",
    "X_train2.columns = ['title','topic_idx']\n",
    "\n",
    "X_train2.to_csv(\"X_train_label2.csv\",index=False, encoding='cp949')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5c8fc326",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-07T00:42:54.729626Z",
     "start_time": "2022-04-07T00:42:54.305443Z"
    }
   },
   "outputs": [],
   "source": [
    "Y_label2_val_int = [Y_label2_val.iloc[i].argmax() for i in tqdm(range(Y_label2_val.shape[0]))]\n",
    "\n",
    "X_val = X_val.reset_index(drop=True)\n",
    "\n",
    "X_val2 = pd.concat([X_val, DataFrame(Y_label2_val_int)], axis=1).reset_index(drop=True)\n",
    "X_val2.columns = ['title','topic_idx']\n",
    "\n",
    "X_val2.to_csv(\"X_val_label2.csv\",index=False, encoding='cp949')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1eb3719",
   "metadata": {},
   "source": [
    "## 맞춤법 교정 + 명사 추출 (checking korean spell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3db1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hanspell import spell_checker\n",
    "\n",
    "spell_total = []\n",
    "\n",
    "for i in tqdm(range(400002,first.shape[0])):\n",
    "    spell_total.append(spell_checker.check(first['total'][i]).as_dict()['checked'])\n",
    "    if i % 50000 == 2:\n",
    "        DataFrame(spell_total).to_csv(\"spell_total.csv\",index=False, encoding='cp949')\n",
    "\n",
    "DataFrame(spell_total).to_csv(\"spell_total.csv\",index=False, encoding='cp949')\n",
    "\n",
    "first['spell_check'] = spell_total\n",
    "\n",
    "first['spell_nouns'] = [ ' '.join([w for w in komoran.nouns(temp) if w not in stop_words]) for temp in tqdm(first['spell_check']) ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04312e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "second_temp = []\n",
    "\n",
    "for i in tqdm(range(second.shape[0])):\n",
    "    second_temp.append( ' '.join([w for w in komoran.nouns(spell_checker.check(second['total'][i]).as_dict()['checked']) if w not in stop_words]) )\n",
    "    if i % 5000 == 2:\n",
    "        DataFrame(second_temp).to_csv(\"second_spell_check.csv\",index=False, encoding='cp949')\n",
    "\n",
    "DataFrame(second_temp).to_csv(\"second_spell_check.csv\",index=False, encoding='cp949')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99eae1e7",
   "metadata": {},
   "source": [
    "## 띄어쓰기(pykospacing, quickspacer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf12650",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pykospacing import Spacing\n",
    "\n",
    "spacing = Spacing()\n",
    "\n",
    "pyspacing = []\n",
    "\n",
    "for i in tqdm(range(first.shape[0])):\n",
    "    pyspacing.append(spacing(first['total'][i]))\n",
    "    if i % 100000 == 1 :\n",
    "        DataFrame(pyspacing).to_csv(\"pyspacing.csv\",index=False, encoding = 'cp949')\n",
    "        \n",
    "DataFrame(pyspacing).to_csv(\"pyspacing.csv\",index=False, encoding = 'cp949')\n",
    "\n",
    "first['pyspacing'] = pyspacing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f29ba15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from quickspacer import Spacer\n",
    "\n",
    "spacer = Spacer()\n",
    "\n",
    "quick = []\n",
    "\n",
    "for i in tqdm(range(first.shape[0])):\n",
    "    quick.append(spacer.space([first['total'][i]])[0])\n",
    "    if i % 100000 == 1 :\n",
    "        DataFrame(quick).to_csv(\"quick_spacing.csv\",index=False, encoding = 'cp949')\n",
    "        \n",
    "DataFrame(quick).to_csv(\"quick_spacing.csv\",index=False, encoding = 'cp949')\n",
    "\n",
    "first['quick_nouns'] = [ ' '.join([w for w in komoran.nouns(temp) if w not in stop_words]) for temp in tqdm(first['quick']) ] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f78b7d",
   "metadata": {},
   "source": [
    "## kkma 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1adf5716",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Kkma\n",
    "\n",
    "kkma = Kkma()\n",
    "\n",
    "stop_words = list(pd.read_csv(\"stopwords.txt\",header=None, names=['stop'])['stop'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d97e9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "kkma_temp = []\n",
    "\n",
    "for i in tqdm(range(first.shape[0])):\n",
    "    kkma_temp.append( ' '.join([w for w in kkma.nouns(first['total'][i]) if w not in stop_words]) )\n",
    "    if i % 100000 == 1 :\n",
    "        DataFrame(kkma_temp).to_csv(\"kkma_temp.csv\",index=False, encoding = 'cp949')\n",
    "        \n",
    "DataFrame(kkma_temp).to_csv(\"kkma_temp.csv\",index=False, encoding = 'cp949')\n",
    "\n",
    "first['kkma_nouns'] = kkma_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e72fd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyspacing_kkma = []\n",
    "\n",
    "for i in tqdm(range(first.shape[0])):\n",
    "    pyspacing_kkma.append( ' '.join([w for w in kkma.nouns(first['pyspacing'][i]) if w not in stop_words]) )\n",
    "    if i % 50000 == 1 :\n",
    "        DataFrame(pyspacing_kkma).to_csv(\"pyspacing_kkma.csv\",index=False, encoding = 'cp949')\n",
    "        \n",
    "DataFrame(pyspacing_kkma).to_csv(\"pyspacing_kkma.csv\",index=False, encoding = 'cp949')\n",
    "\n",
    "first['pyspacing_kkma'] = pyspacing_kkma\n",
    "\n",
    "first.to_csv(\"eda_complete_last_final.csv\",index=False, encoding = 'cp949')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9915dee8",
   "metadata": {},
   "source": [
    "## oversampling(com_ve1_over)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c789e01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "ros = RandomOverSampler(random_state=1217)\n",
    "\n",
    "X_train, Y_label1_train = ros.fit_resample(X_train, Y_label1_train)\n",
    "\n",
    "Y_label1_train = get_dummies(Y_label1_train)\n",
    "\n",
    "Y_label1_val = get_dummies(Y_label1_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3dbbc2",
   "metadata": {},
   "source": [
    "## 이전 분류랑 맞게 선택하는 버전(com_ve1_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a966e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_label1 = label1.predict(X_val, verbose=0)\n",
    "y_val_label1 = [i.argmax() for i in y_val_label1]\n",
    "y_val_label1 = [Y_label1.columns[i] for i in y_val_label1]\n",
    "\n",
    "testing_template = DataFrame(y_val_label1, columns=['digit_1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce1bbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "second_template = first[['digit_1','digit_2']].drop_duplicates(keep='first')\n",
    "second_template = second_template.sort_values(['digit_1','digit_2']).reset_index(drop=True)\n",
    "\n",
    "y_val_label2_prob = label2.predict(X_val, verbose=0)\n",
    "\n",
    "temp2 = Y_label2_val.reset_index(drop=True)\n",
    "y_val2 = []\n",
    "for i in tqdm(range(temp2.shape[0])):\n",
    "    y_val2.append(temp2.iloc[i].argmax())\n",
    "y_val2 = [Y_label2.columns[i] for i in y_val2]\n",
    "\n",
    "second_temp = []\n",
    "for i in tqdm(range(testing_template.shape[0])):\n",
    "    all_total = second_template[second_template['digit_1'] == testing_template['digit_1'][i]]['digit_2']\n",
    "    all_list = list(all_total)\n",
    "    all_index = all_total.index\n",
    "    if testing_template['digit_2'][i] in all_list:\n",
    "        second_temp.append(testing_template['digit_2'][i])\n",
    "    else:\n",
    "        second_temp.append(second_template['digit_2'][all_index[y_val_label2_prob[0][all_index].argmax()]])\n",
    "        \n",
    "print(\"val accuracy : \", accuracy_score(y_val2, second_temp))\n",
    "\n",
    "print(\"val f1 score : \", f1_score(y_val2, second_temp, average = 'macro')) #일반적인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0846ebab",
   "metadata": {},
   "outputs": [],
   "source": [
    "third_template = first[['digit_1','digit_2','digit_3']].drop_duplicates(keep='first')\n",
    "third_template = third_template.sort_values(['digit_1','digit_2','digit_3']).reset_index(drop=True)\n",
    "\n",
    "y_val_label3_prob = label3.predict(X_val, verbose=0)\n",
    "\n",
    "temp2 = Y_label3_val.reset_index(drop=True)\n",
    "y_val2 = []\n",
    "for i in tqdm(range(temp2.shape[0])):\n",
    "    y_val2.append(temp2.iloc[i].argmax())\n",
    "y_val2 = [Y_label3.columns[i] for i in y_val2]\n",
    "\n",
    "third_temp = []\n",
    "for i in tqdm(range(testing_template.shape[0])):\n",
    "    all_total = third_template[(third_template['digit_2'] == second_temp[i])]['digit_3']\n",
    "    all_list = list(all_total)\n",
    "    all_index = all_total.index\n",
    "    if testing_template['digit_3'][i] in all_list:\n",
    "        third_temp.append(testing_template['digit_3'][i])\n",
    "    else:\n",
    "        third_temp.append(third_template['digit_3'][all_index[y_val_label3_prob[0][all_index].argmax()]])\n",
    "        \n",
    "print(\"val accuracy : \", accuracy_score(y_val2, third_temp))\n",
    "\n",
    "print(\"val f1 score : \", f1_score(y_val2, third_temp, average = 'macro')) #일반적인"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b320a1d",
   "metadata": {},
   "source": [
    "## 모델2 미 사용 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bfd3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "first['digit_2'] = [int(str(i)[:-1]) for i in first['digit_3']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a54cf6",
   "metadata": {},
   "source": [
    "## 산업 분류 활용 (more eda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b663325f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-08T14:20:59.412940Z",
     "start_time": "2022-08-08T14:20:59.407938Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "check = pd.read_excel(r\"C:\\Users\\10331\\Documents\\GitHub\\statistic_work\\참고자료\\한국표준산업분류(10차)_국문.xlsx\", header = 2)\n",
    "\n",
    "check.columns = ['digit_1','one','digit_2','two','digit_3','three', 'digit_4','four','digit_5','five',]\n",
    "\n",
    "for j in range(check.shape[1]):\n",
    "    for i in range(1, check.shape[0]):\n",
    "        if pd.isna(check[check.columns[j]][i]) == True:\n",
    "            check[check.columns[j]][i] = check[check.columns[j]][i-1]\n",
    "\n",
    "check2 = check2[['digit_1','digit_2','digit_3','two','three','four','five']]\n",
    "\n",
    "check2 = check2[(check2['digit_1']!='T') & (check2['digit_1']!='U') ]\n",
    "\n",
    "check2['total'] = check2['two'] + ' ' + check2['three'] + ' ' + check2['four'] + ' ' + check2['five']\n",
    "\n",
    "check2['total'] = check2['total'].apply((lambda x: re.sub(',',' ',x)))\n",
    "check2['total'] = check2['total'].apply((lambda x: re.sub('-',' ',x)))\n",
    "check2['total'] = check2['total'].apply((lambda x: re.sub('[^a-zA-z0-9\\s가-힣]','',x)))\n",
    "check2['total'] = check2['total'].apply((lambda x: re.sub('  ',' ',x)))\n",
    "\n",
    "check2['nouns'] = [' '.join([w for w in komoran.nouns(temp) if w not in stop_words]) for temp in tqdm(check2['total'])]\n",
    "\n",
    "check2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d127fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_first = tokenizer.texts_to_sequences(check2['nouns'].values)\n",
    "X_first = pad_sequences(X_first, maxlen=28)\n",
    "\n",
    "Y_label1_first = get_dummies(check2['digit_1'])\n",
    "\n",
    "X_first.shape, Y_label1_first.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b5f919",
   "metadata": {},
   "source": [
    "## not unique(train만 미 중복)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9891a4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "first = pd.read_csv(\"eda_complete_all.csv\", encoding = 'cp949')\n",
    "first.nouns=first.nouns.astype(str)\n",
    "\n",
    "first_unique = first.copy()\n",
    "del first_unique['AI_id']\n",
    "\n",
    "first_unique = first_unique.drop_duplicates(keep='first').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2847e5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y_label1 = get_dummies(first_unique['digit_1'])\n",
    "\n",
    "Y_label1 = first_unique['digit_1']\n",
    "\n",
    "X_train, X_val, Y_label1_train, Y_label1_val =  train_test_split(first_unique[['total','nouns']], Y_label1, test_size = 0.15, random_state = 1111)\n",
    "\n",
    "val_check = pd.concat([X_val,Y_label1_val],axis=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d2978e",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_check = first.copy()\n",
    "first_check = first_check[['total','nouns','digit_1']].reset_index(drop=True)\n",
    "\n",
    "checking = pd.merge(first_check, val_check, how='left', on=['total','digit_1'], indicator=True)\n",
    "\n",
    "first_check = checking[checking['_merge']=='left_only'].reset_index(drop=True)\n",
    "del first_check['_merge']\n",
    "del first_check['nouns_y']\n",
    "first_check.columns = ['total','nouns','digit_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31ef9458",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-08T14:21:06.783732Z",
     "start_time": "2022-08-08T14:21:06.779758Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_fatures = 10000\n",
    "tokenizer = Tokenizer(num_words=max_fatures, split=' ') #num_words 단어 개수 제한\n",
    "\n",
    "tokenizer.fit_on_texts(first_check['nouns'].values)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(first_check['nouns'].values)\n",
    "X_train = pad_sequences(X_train)\n",
    "\n",
    "Y_label1_train = get_dummies(first_check['digit_1'])\n",
    "\n",
    "X_val = tokenizer.texts_to_sequences(val_check['nouns'].values)\n",
    "X_val = pad_sequences(X_val, maxlen=28)\n",
    "\n",
    "Y_label1_val = get_dummies(val_check['digit_1'])\n",
    "\n",
    "X_train.shape, X_val.shape, Y_label1_train.shape, Y_label1_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6313528a",
   "metadata": {},
   "source": [
    "## class weight 부여(practice_complex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975f197d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def create_class_weight(labels_dict,mu=0.15):\n",
    "    total = np.sum(list(labels_dict.values()))\n",
    "    keys = labels_dict.keys()\n",
    "    class_weight = dict()\n",
    "    \n",
    "    for key in keys:\n",
    "        score = math.log(mu*total/float(labels_dict[key]))\n",
    "        class_weight[key] = score if score > 1.0 else 1.0\n",
    "    \n",
    "    return class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a028c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_weight = create_class_weight(Counter(y_train))\n",
    "\n",
    "temp = DataFrame([y_weight.keys(),y_weight.values()]).T\n",
    "temp.columns=['key','value']\n",
    "temp2 = temp.sort_values('key').reset_index(drop=True)\n",
    "\n",
    "temp3 = dict(temp2.values)\n",
    "\n",
    "for i in range(19):\n",
    "    temp3[i] = temp3.pop(temp2['key'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2873d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 중간 내용 생략\n",
    "    \n",
    "score = model.fit(X_train, y_train, epochs = 10, batch_size=128, validation_data=(X_val, y_val), callbacks=[earlyStop],)\n",
    "                 class_weight = temp3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67944c5f",
   "metadata": {},
   "source": [
    "## learning rate 자동 조절(using complex lstms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c1ae60",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LRSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, init_lr, warmup_epoch,\n",
    "                 steps_per_epoch,\n",
    "                 decay_fn, *,\n",
    "                 continue_epoch = 0):\n",
    "        self.init_lr = init_lr\n",
    "        self.decay_fn = decay_fn\n",
    "        self.warmup_epoch = warmup_epoch\n",
    "        self.continue_epoch = continue_epoch\n",
    "        self.steps_per_epoch = steps_per_epoch\n",
    "        self.lr = 1e-4 # remove\n",
    "\n",
    "    def on_epoch_begin(self, epoch):\n",
    "        epoch = tf.cast(epoch, tf.float64)\n",
    "        \n",
    "        global_epoch = tf.cast(epoch + 1, tf.float64)\n",
    "        warmup_epoch_float = tf.cast(self.warmup_epoch, tf.float64)\n",
    "        \n",
    "        lr = tf.cond(\n",
    "            global_epoch < warmup_epoch_float,\n",
    "            lambda: tf.cast(self.init_lr * (global_epoch / warmup_epoch_float), tf.float64),\n",
    "            lambda: tf.cast(self.decay_fn(epoch - warmup_epoch_float), tf.float64)\n",
    "        )\n",
    "        self.lr = lr\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        def compute_epoch(step):\n",
    "            return step // self.steps_per_epoch\n",
    "        \n",
    "        epoch = compute_epoch(step)\n",
    "        epoch = epoch + self.continue_epoch\n",
    "        \n",
    "        self.on_epoch_begin(epoch)\n",
    "        \n",
    "        return self.lr\n",
    "\n",
    "def get_steps(x_size, batch_size):\n",
    "    if x_size / batch_size == 0:\n",
    "        return x_size // batch_size\n",
    "    else:\n",
    "        return x_size // batch_size + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9411a10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_size: train_set 크기\n",
    "data_size = 562148\n",
    "BATCH_SIZE = 512\n",
    "EPOCHS = 500\n",
    "warmup_epoch = int(EPOCHS * 0.01)\n",
    "init_lr = 0.0001\n",
    "min_lr = 1e-7\n",
    "power = 1.\n",
    "    \n",
    "lr_scheduler = tf.keras.optimizers.schedules.PolynomialDecay(\n",
    "    initial_learning_rate = init_lr,\n",
    "    decay_steps = EPOCHS - warmup_epoch,\n",
    "    end_learning_rate = min_lr,\n",
    "    power = power\n",
    ")\n",
    "\n",
    "# get_steps: epoch당 전체 step 수 계산\n",
    "lr_schedule = LRSchedule(init_lr, warmup_epoch,\n",
    "                         steps_per_epoch = get_steps(data_size, BATCH_SIZE),\n",
    "                         decay_fn = lr_scheduler,\n",
    "                         continue_epoch = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b595125",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 128\n",
    "lstm_out = 196\n",
    "\n",
    "# earlyStop=EarlyStopping(monitor=\"val_accuracy\",mode = 'max', patience=3, verbose=1) #최소 3번 이상의 에포치 동안 개선 없을시\n",
    "earlyStop=EarlyStopping(monitor=\"val_accuracy\",patience=10, verbose=1) #최소 3번 이상의 에포치 동안 개선 없을시\n",
    "\n",
    "adam = Adam(\n",
    "    learning_rate=lr_schedule, # default = 0.001 (줄인게 효과는 있어보임)\n",
    "    beta_1=0.9,\n",
    "    beta_2=0.999,\n",
    "    epsilon=1e-07,\n",
    "    amsgrad=False,\n",
    "    name='Adam'\n",
    ")\n",
    "\n",
    "label1_f1 = tfa.metrics.F1Score(19,'macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae33dc7d",
   "metadata": {},
   "source": [
    "## bn & cnn-lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1846c8e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-08T14:21:22.110148Z",
     "start_time": "2022-08-08T14:21:22.096935Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#집 버전\n",
    "\n",
    "label1 = Sequential()\n",
    "label1.add(Embedding(max_fatures, embed_dim,input_length = X.shape[1]))\n",
    "\n",
    "label1.add(Conv1D(filters=64, kernel_size=3, padding='same', activation='relu'))\n",
    "label1.add(BatchNormalization())\n",
    "label1.add(Activation('relu'))\n",
    "\n",
    "label1.add(MaxPooling1D(pool_size=2))\n",
    "label1.add(BatchNormalization())\n",
    "label1.add(Activation('relu'))\n",
    "\n",
    "label1.add(SpatialDropout1D(0.4))\n",
    "\n",
    "label1.add(CuDNNLSTM(lstm_out))\n",
    "label1.add(BatchNormalization())\n",
    "label1.add(Activation('relu'))\n",
    "\n",
    "# label1.add(CuDNNLSTM(lstm_out))\n",
    "\n",
    "label1.add(Dense(units=19, activation = 'softmax'))\n",
    "\n",
    "label1.compile(loss = 'categorical_crossentropy', optimizer=adam,metrics = ['accuracy', label1_f1])\n",
    "\n",
    "label1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61f73e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = label1.fit(X_train, Y_label1_train, epochs = 500, batch_size=512, validation_data=(X_val, Y_label1_val), \n",
    "                   callbacks=[earlyStop], )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3270ca0",
   "metadata": {},
   "source": [
    "## bi-lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af774e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#집 버전\n",
    "\n",
    "label1 = Sequential()\n",
    "\n",
    "label1.add(Embedding(max_fatures, embed_dim,input_length = X.shape[1]))\n",
    "\n",
    "label1.add(Conv1D(filters=64, kernel_size=3, padding='same', activation='relu'))\n",
    "label1.add(BatchNormalization())\n",
    "\n",
    "label1.add(MaxPooling1D(pool_size=2))\n",
    "label1.add(BatchNormalization())\n",
    "\n",
    "label1.add(Bidirectional(CuDNNLSTM(lstm_out, return_sequences=True)))\n",
    "label1.add(BatchNormalization())\n",
    "label1.add(Activation('relu'))\n",
    "\n",
    "label1.add(Bidirectional(CuDNNLSTM(98)))\n",
    "label1.add(BatchNormalization())\n",
    "label1.add(Activation('relu'))\n",
    "\n",
    "label1.add(Dense(units=19, activation = 'softmax'))\n",
    "\n",
    "label1.compile(loss = 'categorical_crossentropy', optimizer=adam,metrics = ['accuracy', label1_f1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0f6d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = label1.fit(X_train, Y_label1_train, epochs = 500, batch_size=512, validation_data=(X_val, Y_label1_val), \n",
    "                   callbacks=[earlyStop], )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df41287e",
   "metadata": {},
   "source": [
    "## attention(sequential 자체가 linear 한거임)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b531d286",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-08T14:21:30.772308Z",
     "start_time": "2022-08-08T14:21:30.759306Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras_self_attention import SeqSelfAttention\n",
    "from keras.layers import Flatten\n",
    "\n",
    "#회사 버전\n",
    "\n",
    "label1 = Sequential()\n",
    "label1.add(Embedding(max_fatures, embed_dim,input_length = 28))\n",
    "label1.add(SpatialDropout1D(0.2))\n",
    "label1.add(CuDNNLSTM(lstm_out,  return_sequences=True))\n",
    "label1.add(SeqSelfAttention(attention_activation='softmax'))\n",
    "label1.add(Flatten())\n",
    "label1.add(Dense(units=19, activation = 'softmax'))\n",
    "label1.compile(loss = 'categorical_crossentropy', optimizer=adam,metrics = ['accuracy', label1_f1])\n",
    "\n",
    "label1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8c4ae4",
   "metadata": {},
   "source": [
    "## plot_model(딥러닝 시각화) (실행 안됨)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bbffd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "import pydot\n",
    "\n",
    "plot_model(label1, to_file='model_shapes.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e762c0",
   "metadata": {},
   "source": [
    "## nouns unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99d738f",
   "metadata": {},
   "outputs": [],
   "source": [
    "first['nouns'] = [' '.join(list(set([w for w in komoran.nouns(\n",
    "    temp) if w not in stop_words]))) for temp in tqdm(first['total'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e5f2e0",
   "metadata": {},
   "source": [
    "## randomforest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae94f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators = 100)\n",
    "\n",
    "rf.fit(X_train, Y_label1_train)\n",
    "\n",
    "# rf.score(X_train, Y_label1_train)\n",
    "\n",
    "# rf.score(X_val, Y_label1_val)\n",
    "\n",
    "# y_pred_rf = rf.predict(X_val)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "398.812px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
